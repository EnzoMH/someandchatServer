{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyimF4i_4BRs"
      },
      "source": [
        "## 1. Library Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "f57FFUJj4BRu"
      },
      "outputs": [],
      "source": [
        "import nltk # 텍스트 데이터를 처리\n",
        "import numpy as np # 말뭉치를 배열로 표현\n",
        "import random\n",
        "import operator\n",
        "import string # 표준 파이썬 문자열을 처리\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity # 이를 나중에 사용하여 두 개의 문장이 얼마나 비슷한지를 결정합니다.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Experience 2에서 단어 가방을 만드는 함수를 만들었던 것을 기억하십니까? 이 함수는 같은 일을 합니다!\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression  # LogisticRegression import 추가\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCuEVCo-4BRw"
      },
      "source": [
        "## 2. Data Uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZO8kByo4BRw",
        "outputId": "c603c862-1568-4a5c-a819-142dbba59983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading text files for the label: 0\n",
            "Loading text files for the label: 25\n",
            "Loading text files for the label: 50\n",
            "Loading text files for the label: 75\n",
            "Loading text files for the label: 100\n"
          ]
        }
      ],
      "source": [
        "  # Colab에서 Google 드라이브를 마운트하기 위한 라이브러리\n",
        "\n",
        "# 구글 드라이브 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def loadfile(path):\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for label in ('0', '25', '50', '75', '100'):\n",
        "        print(\"Loading text files for the label: \" + label)\n",
        "\n",
        "        label_path = os.path.join(path, label)\n",
        "        for filename in os.listdir(label_path):\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(os.path.join(label_path, filename), 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "\n",
        "                # 레이블을 숫자로 변환\n",
        "                if label == '0':\n",
        "                    Y.append(0)\n",
        "                elif label == '25':\n",
        "                    Y.append(1)\n",
        "                elif label == '50':\n",
        "                    Y.append(2)\n",
        "                elif label == '75':\n",
        "                    Y.append(3)\n",
        "                elif label == '100':\n",
        "                    Y.append(4)\n",
        "\n",
        "                # 텍스트 데이터를 X에 추가\n",
        "                X.append(text)\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "# 경로를 구글 드라이브 경로로 변경\n",
        "directory_path = '/content/drive/MyDrive/likeability_Son/'\n",
        "\n",
        "# loadfile 함수 호출\n",
        "X, Y = loadfile(directory_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIa702rGVyCL",
        "outputId": "c4ca0dc2-c3f7-427a-9716-035f3ec449a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (500,)\n",
            "Y shape: (500,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X shape:\", X.shape)\n",
        "print(\"Y shape:\", Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux0_bAitWCCS",
        "outputId": "08a6fcaa-b2e1-4727-bc8d-aefcda55ba74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of .txt files in label 0: 100\n",
            "Number of .txt files in label 25: 100\n",
            "Number of .txt files in label 50: 100\n",
            "Number of .txt files in label 75: 100\n",
            "Number of .txt files in label 100: 100\n"
          ]
        }
      ],
      "source": [
        "def count_txt_files(path):\n",
        "    file_counts = {}\n",
        "\n",
        "    for label in ('0', '25', '50', '75', '100'):\n",
        "        label_path = os.path.join(path, label)\n",
        "        txt_files = [filename for filename in os.listdir(label_path) if filename.endswith('.txt')]\n",
        "        file_counts[label] = len(txt_files)\n",
        "\n",
        "    return file_counts\n",
        "\n",
        "# 경로를 구글 드라이브 경로로 변경\n",
        "directory_path = '/content/drive/MyDrive/likeability_Son/'\n",
        "\n",
        "# .txt 파일 수 확인\n",
        "file_counts = count_txt_files(directory_path)\n",
        "\n",
        "# 결과 출력\n",
        "for label, count in file_counts.items():\n",
        "    print(f\"Number of .txt files in label {label}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlECzPTTYyT5"
      },
      "source": [
        "## 3. Data Refining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aaXFwfltZCwn"
      },
      "outputs": [],
      "source": [
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^가-힝A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\":\", \" : \", string)  # 콜론은 다른 문자로 대체하지 않음\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_lyYEhTuvz58"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # 소문자화\n",
        "    text = re.sub(r'[.:]', '', text)  # 특정 기호 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 공백 정규화\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_data_and_labels(directory_path):\n",
        "    conversations = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('0', '25', '50', '75', '100'):\n",
        "        label_path = os.path.join(directory_path, label)\n",
        "        for filename in os.listdir(label_path):\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(os.path.join(label_path, filename), 'r', encoding='utf-8') as file:\n",
        "                    file_content = file.read()\n",
        "                    conversations.append(file_content)\n",
        "                    labels.append(int(label))\n",
        "\n",
        "    return conversations, labels\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/likeability_Son/'\n",
        "conversations, labels = load_data_and_labels(directory_path)\n",
        "\n",
        "print(f'Number of conversations: {len(conversations)}')\n",
        "print(f'Number of labels: {len(labels)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_PencWd3Bc9",
        "outputId": "3f7bab1c-0daf-46df-c2f8-5b0ff9372d78"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of conversations: 500\n",
            "Number of labels: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X shape:\", X.shape)\n",
        "print(\"Y shape:\", Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96zfgk0S3JK6",
        "outputId": "86482609-b72e-4fc3-93a4-1a9ae6304524"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (500,)\n",
            "Y shape: (500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S_NXGLG43IrS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eli_2pxczaq-"
      },
      "source": [
        "## 4. 호감도 가중치 증가 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWbISKOOweHx"
      },
      "source": [
        "4-1. 이모지 사용 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_nz2COlnwABg"
      },
      "outputs": [],
      "source": [
        "def count_emojis(text):\n",
        "    emoji_pattern = re.compile('['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        ']', flags=re.UNICODE)\n",
        "    return len(emoji_pattern.findall(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xFjZQMGwgPz"
      },
      "source": [
        "4.2. 대화 양방향성 관련 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mijm9AY8wcn8"
      },
      "outputs": [],
      "source": [
        "def check_bidirectional_conversation(text):\n",
        "    a_contributions = len(re.findall(r'A:', text))\n",
        "    b_contributions = len(re.findall(r'B:', text))\n",
        "    return a_contributions > 0 and b_contributions > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-PEUuouZQnR"
      },
      "source": [
        "4-3. 답장 속도 기반 호감도 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GBJjqzhK4BRx"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# 대화 데이터를 .txt 파일에서 불러오기\n",
        "conversation = []\n",
        "for label in ('0', '25', '50', '75', '100'):\n",
        "    label_path = os.path.join(directory_path, label)\n",
        "    for filename in os.listdir(label_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(label_path, filename), 'r', encoding='utf-8') as file:\n",
        "                conversation.extend(file.readlines())\n",
        "\n",
        "# 호감도 레이블 생성 함수\n",
        "def create_likeability_labels(conversation):\n",
        "    response_times = []\n",
        "    a_message_time = None  # A의 메시지 시간\n",
        "    b_message_time = None  # B의 메시지 시간\n",
        "\n",
        "    for line in conversation:\n",
        "        if 'A:' in line:\n",
        "            time_str = re.search(r'\\((\\d{2}):(\\d{2})\\)', line)\n",
        "            if time_str:\n",
        "                hours, minutes = map(int, time_str.groups())\n",
        "                a_message_time = hours * 60 + minutes\n",
        "\n",
        "        if 'B:' in line and a_message_time is not None:\n",
        "            time_str = re.search(r'\\((\\d{2}):(\\d{2})\\)', line)\n",
        "            if time_str:\n",
        "                hours, minutes = map(int, time_str.groups())\n",
        "                b_message_time = hours * 60 + minutes\n",
        "                response_time = b_message_time - a_message_time\n",
        "                response_times.append(response_time)\n",
        "\n",
        "    # 호감도 레이블 생성\n",
        "    likeability_labels = []\n",
        "    for response_time in response_times:\n",
        "        if response_time <= 10:\n",
        "            likeability = 100\n",
        "        elif response_time <= 30:\n",
        "            likeability = 75\n",
        "        elif response_time <= 60:\n",
        "            likeability = 50\n",
        "        elif response_time <= 180:\n",
        "            likeability = 25\n",
        "        else:\n",
        "            likeability = 0\n",
        "        likeability_labels.append(likeability)\n",
        "\n",
        "    return likeability_labels\n",
        "\n",
        "# 대화 데이터를 기반으로 호감도 레이블 생성\n",
        "likeability_labels = create_likeability_labels(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# 대화 데이터 불러오기 및 정제\n",
        "conversations = []\n",
        "for label in ('0', '25', '50', '75', '100'):\n",
        "    label_path = os.path.join(directory_path, label)\n",
        "    for filename in os.listdir(label_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(label_path, filename), 'r', encoding='utf-8') as file:\n",
        "                for line in file:\n",
        "                    conversations.append(clean_str(line))\n",
        "\n",
        "# 추가 특성 추출 (이모티콘 사용 및 양방향 대화)\n",
        "emoji_counts = [count_emojis(conv) for conv in conversations]\n",
        "bidirectional = [check_bidirectional_conversation(conv) for conv in conversations]\n",
        "\n",
        "# 토크나이징 및 시퀀스 변환\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(conversations)\n",
        "sequences = tokenizer.texts_to_sequences(conversations)\n",
        "\n",
        "# 패딩\n",
        "data = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# 호감도 레이블 생성\n",
        "likeability_labels = create_likeability_labels(conversations)\n",
        "\n",
        "# 데이터 분할 (Train:Test:Validation = 7:2:1)\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(data, likeability_labels, test_size=0.3, random_state=42)\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_temp, Y_temp, test_size=1/3, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "j_9VMqZN0mBH",
        "outputId": "a87789cd-b6c4-4d6e-b3bc-0f9bf32ea595"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b4c67c40eb74>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# 데이터 분할 (Train:Test:Validation = 7:2:1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikeability_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10750, 0]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mdg09C7wTrq"
      },
      "source": [
        "## 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Hc81XOCPzubM"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 64, input_length=100))\n",
        "model.add(LSTM(1024, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(LSTM(512, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(LSTM(256, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64, kernel_regularizer=l1_l2(l1=0.01, l2=0.01))) # 마지막 LSTM 레이어\n",
        "model.add(Dense(5, activation='softmax')) # 5개의 클래스 (0, 25, 50, 75, 100)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 분할\n",
        "# 먼저 전체 데이터를 Train (70%)과 나머지 (30%)로 분할\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 나머지 데이터를 다시 Test (2/3 of 30%)와 Validation (1/3 of 30%)으로 분할\n",
        "# 최종 비율: Train 70%, Test 20%, Validation 10%\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_temp, Y_temp, test_size=1/3, random_state=42)\n",
        "\n",
        "# 모델 훈련\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=20,\n",
        "    batch_size=4,\n",
        "    validation_data=(X_val, Y_val)\n",
        ")\n",
        "\n",
        "# 훈련 결과 출력\n",
        "print(\"Training Loss: \", history.history['loss'])\n",
        "print(\"Training Accuracy: \", history.history['accuracy'])\n",
        "print(\"Validation Loss: \", history.history['val_loss'])\n",
        "print(\"Validation Accuracy: \", history.history['val_accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "QG_5oIeuwpBN",
        "outputId": "68b1aaf8-7700-4730-95ab-da8e62cd0dba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-013638e24190>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_8QilZBw-ss"
      },
      "source": [
        "## 5.평가지표"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSgRtbaxiajD",
        "outputId": "2f3420ad-0eef-462a-e7ad-507f78f7611a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 4ms/step\n",
            "Accuracy: 0.28\n",
            "F1 Score: 0.12250000000000001\n",
            "Precision: 0.07840000000000001\n",
            "Recall: 0.28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    return accuracy, f1, precision, recall\n",
        "\n",
        "# 테스트 데이터에 대한 예측 수행\n",
        "# 여기서 'model'은 이미 훈련된 모델을 나타냅니다.\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# 모델 평가\n",
        "accuracy = accuracy_score(Y_test, y_pred_classes)\n",
        "f1 = f1_score(Y_test, y_pred_classes, average='weighted')\n",
        "precision = precision_score(Y_test, y_pred_classes, average='weighted')\n",
        "recall = recall_score(Y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "# 결과 출력\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QSAggh7zm_6"
      },
      "source": [
        "## 6. 시각화 그래프"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phk2nialiwqd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(metrics):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics['accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics['loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCjy5nLGiyxl"
      },
      "outputs": [],
      "source": [
        "# plot_metrics(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ab6779"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# # LSTM 모델 구축\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(10000, 64, input_length=100))\n",
        "# model.add(LSTM(128))\n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "927d296d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from tensorflow.keras.layers import GRU\n",
        "\n",
        "# # GRU 모델 구축\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(10000, 64, input_length=100))\n",
        "# model.add(GRU(128))\n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e7d0c97"
      },
      "outputs": [],
      "source": [
        "\n",
        "# BERT 모델 구현 예시 코드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ccf254e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# import numpy as np\n",
        "\n",
        "# # 가정: `texts`는 대화 데이터셋의 원본 텍스트를 포함하는 리스트\n",
        "# # 토크나이징 및 시퀀스로 변환\n",
        "# tokenizer = Tokenizer(num_words=10000)\n",
        "# tokenizer.fit_on_texts(texts)\n",
        "# sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# # 패딩\n",
        "# data = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# # 데이터셋을 훈련, 테스트, 검증 세트로 분할\n",
        "# X_train, X_temp, Y_train, Y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "# X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=2/3, random_state=42)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}